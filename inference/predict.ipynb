{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model.custom_model import CustomAudioCLIP\n",
    "from prediction_scripts._utils import AudioList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initModel(mpath):\n",
    "    m = CustomAudioCLIP(num_target_classes=2).load_from_checkpoint(mpath, num_target_classes=2)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = initModel(\"/app/assets/ckpt-epoch=21-val_loss=0.12-lr=0.005.ckpt\")\n",
    "torch.save(m, \"/app/assets/anode.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER SCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import itertools\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from prediction_scripts._utils import openAudioFile, splitSignal\n",
    "\n",
    "class AudioList():\n",
    "\n",
    "    def __init__(self, length_segments = 3, sample_rate=44100):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.length_segments = length_segments\n",
    "\n",
    "    def read_audio(self, audio_path):\n",
    "        \"\"\"Read the audio, change the sample rate and randomly pick one channel\"\"\"\n",
    "        sig, _ = openAudioFile(audio_path, sample_rate=self.sample_rate)\n",
    "        return sig\n",
    "\n",
    "    def split_segment(self, array):\n",
    "        splitted_array = splitSignal(array, rate=self.sample_rate, seconds=self.length_segments, overlap=0, minlen=3)\n",
    "        return splitted_array\n",
    "\n",
    "    def get_labels(self, splitted_list, label):\n",
    "        arrays_label = []\n",
    "        for array in splitted_list:\n",
    "            array_label = (array, label)\n",
    "            arrays_label.append(array_label)\n",
    "        return arrays_label\n",
    "\n",
    "    def get_processed_list(self, audio_path):\n",
    "\n",
    "        list_segments = []\n",
    "\n",
    "        for item in audio_path:\n",
    "            track = self.read_audio(item)        \n",
    "            label = item.split(\"/\")[-2]\n",
    "            list_divided = self.split_segment(track)\n",
    "            list_arr_label = self.get_labels(list_divided, label)\n",
    "            list_segments.append(list_arr_label)\n",
    "        return list_segments\n",
    "\n",
    "class AudioLoader(Dataset):\n",
    "    def __init__(self, list_data, label_encoder, sr=44100, transform=None):\n",
    "        self.data = list_data\n",
    "        self.label_encoder = label_encoder\n",
    "        self.transform = transform\n",
    "        self.sr=sr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_data(self, data):\n",
    "\n",
    "        array, label = data\n",
    "        array = array.reshape(1, -1)\n",
    "        array = torch.tensor(array)\n",
    "\n",
    "        label_encoded = self.label_encoder.one_hot_sample(label)\n",
    "        label_class = torch.argmax(label_encoded)\n",
    "\n",
    "        return (array, label_class)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        tensor, label = self.process_data(self.data[idx])\n",
    "        return tensor, label\n",
    "\n",
    "    def get_labels(self):\n",
    "        list_labels = []\n",
    "        for x,y in self.data:\n",
    "            list_labels.append(y)\n",
    "        return list_labels\n",
    "\n",
    "class EncodeLabels():\n",
    "    \"\"\"\n",
    "    Function that encodes names of folders as numerical labels\n",
    "    Wrapper around sklearn's LabelEncoder\n",
    "    \"\"\"\n",
    "    def __init__(self, path_to_folders):\n",
    "        self.path_to_folders = path_to_folders\n",
    "        self.class_encode = LabelEncoder()\n",
    "        self._labels_name()\n",
    "\n",
    "    def _labels_name(self):\n",
    "        labels = glob.glob(self.path_to_folders + \"/*\")\n",
    "        labels = [l.split(\"/\")[-1] for l in labels]\n",
    "        self.class_encode.fit(labels)\n",
    "        \n",
    "    def __getLabels__(self):\n",
    "        return self.class_encode.classes_\n",
    "\n",
    "    def to_one_hot(self, codec, values):\n",
    "        value_idxs = codec.transform(values)\n",
    "        return torch.eye(len(codec.classes_))[value_idxs]\n",
    "\n",
    "    def one_hot_sample(self, label):\n",
    "        t_label = self.to_one_hot(self.class_encode, [label])\n",
    "        return t_label\n",
    "\n",
    "def initModel(model_path):\n",
    "    m = torch.load(model_path).eval()\n",
    "    m_q = quantize_dynamic(m, qconfig_spec={torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8)\n",
    "    return m_q\n",
    "    \n",
    "def getPredLoader(list_arrays, l):\n",
    "    list_preds = AudioLoader(list_arrays, label_encoder)\n",
    "    predLoader = DataLoader(list_preds, batch_size=1, num_workers=4, pin_memory=False)\n",
    "    return predLoader\n",
    "\n",
    "def predict(testLoader, model):\n",
    "\n",
    "    proba_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for array, label in testLoader:\n",
    "        tensor = torch.tensor(array)\n",
    "        output = model(tensor)\n",
    "        output = np.exp(output.detach().numpy())\n",
    "        proba_list.append(output[0][1])\n",
    "        label_list.append(label[0])\n",
    "\n",
    "    return (np.array(proba_list), np.array(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/prediction_scripts/config.yaml\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################\n",
    "# Get the dataset #\n",
    "###################\n",
    "\n",
    "allFiles = [f for f in glob.glob(config[\"INPUT_PATH\"] + \"/**/*\", recursive=True) if os.path.isfile(f)]\n",
    "allFiles = [f for f in allFiles if f.endswith( (\".WAV\", \".wav\", \".mp3\") )]\n",
    "\n",
    "# Instantiate the audio iterator class - cut the audio into segments\n",
    "audio_list= AudioList(length_segments=config[\"SIG_LENGTH\"], sample_rate=config[\"SAMPLE_RATE\"])\n",
    "\n",
    "list_test = audio_list.get_processed_list(allFiles)\n",
    "list_test = list(itertools.chain.from_iterable(list_test))\n",
    "\n",
    "###########################\n",
    "# Create the labelEncoder #\n",
    "###########################\n",
    "label_encoder = EncodeLabels(path_to_folders=config[\"INPUT_PATH\"])\n",
    "\n",
    "# Save name of the folder and associated label in a json file\n",
    "l = label_encoder.__getLabels__()\n",
    "t = label_encoder.class_encode.transform(l)\n",
    "\n",
    "folder_labels = []\n",
    "for i, j in zip(l,t):\n",
    "    item = {\"Folder\": i, \"Label\": int(j)}\n",
    "    folder_labels.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9453/222997309.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(array)\n"
     ]
    }
   ],
   "source": [
    "audioloader = AudioLoader(list_test, label_encoder)\n",
    "predLoader = DataLoader(audioloader, batch_size=1, num_workers=4, pin_memory=False)\n",
    "model = initModel(config[\"MODEL\"])\n",
    "proba_list, labels = predict(predLoader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(labels, proba_list)\n",
    "thresholds = np.append(thresholds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96888715, 0.97043675, 0.97414303, 0.97651154, 0.9881323 ,\n",
       "       0.98857063, 0.99071723, 0.99147403, 0.9926458 , 0.99370641,\n",
       "       0.9941631 , 0.99424696, 0.99455768, 0.99456263, 0.99588394,\n",
       "       0.99641901, 0.99698162, 0.9971925 , 0.9974699 , 0.99755955,\n",
       "       0.9975943 , 0.99800658, 0.99805939, 0.99812984, 0.9981305 ,\n",
       "       0.99817747, 0.99863452, 0.99890178, 1.        ])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'thresholds': thresholds, 'precision': precision, 'recall': recall}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.12 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.12 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Data/audioCLIP/results/YELLMACS_20120131_105013.csv', '/Data/audioCLIP/results/YELLPAYP_20130124_083717.csv', '/Data/audioCLIP/results/YELLMJ2B_20140327_154332.csv', '/Data/audioCLIP/results/YELLSYL3_20130206_223122.csv', '/Data/audioCLIP/results/YELLMM8K_20080214_021244.csv', '/Data/audioCLIP/results/YELLSYL3_20130207_075037.csv', '/Data/audioCLIP/results/YELLFOPP_20141229_110225.csv', '/Data/audioCLIP/results/YELLMJ23_20111229_154155.csv']\n",
      "275\n",
      "808\n",
      "37\n",
      "0\n",
      "0\n",
      "13\n",
      "568\n",
      "578\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = '/Data/audioCLIP/results' # use your path\n",
    "all_files = glob.glob(path + \"/*\")\n",
    "print(all_files)\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df_conf = df[df[\"confidence\"] > 0.99]\n",
    "    print(len(df_conf))\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.3 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
      "  configuration: --prefix=/opt/conda --cc=/opt/conda/conda-bld/ffmpeg_1597178665428/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "\u001b[0;35m[mp3 @ 0x55be63d6ccc0] \u001b[0m\u001b[0;33mEstimating duration from bitrate, this may be inaccurate\n",
      "\u001b[0mInput #0, mp3, from '/Data/audioCLIP/files_to_split/YELLCRPA_20110112_060550.MP3':\n",
      "  Duration: 06:12:49.67, start: 0.000000, bitrate: 96 kb/s\n",
      "    Stream #0:0: Audio: mp3, 44100 Hz, stereo, fltp, 96 kb/s\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output000.mp3' for writing\n",
      "Output #0, segment, to '/Data/audioCLIP/output%03d.mp3':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0: Audio: mp3, 44100 Hz, stereo, fltp, 96 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (copy)\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output001.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output002.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output003.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output004.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output005.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output006.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output007.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output008.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output009.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output010.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output011.mp3' for writing\n",
      "\u001b[1;35m[segment @ 0x55be63d71a00] \u001b[0mOpening '/Data/audioCLIP/output012.mp3' for writing\n",
      "size=N/A time=06:12:49.64 bitrate=N/A speed=3.6e+03x    \n",
      "video:0kB audio:262145kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
     ]
    }
   ],
   "source": [
    "! ffmpeg -i /Data/audioCLIP/files_to_split/YELLCRPA_20110112_060550.MP3 -c copy -map 0 -segment_time 00:30:00 -f segment /Data/audioCLIP/output%03d.mp3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
