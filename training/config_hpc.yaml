# training_parameters:
current_experiment: &current_exp experiment_ray
train_val_dataset_path: "/data/**"
path_lightning_metrics: "."
proportion_training: 0.8 # fraction of the data to use for validation
batch_size: 32 #tune.choice([32, 64, 128]) # batch size used // DECREASE IF SHARED MEMORY PROBLEM
n_epoch: 500 # number of epochs to run, due to stopping rule this number just needs to be big
stopping_rule_patience: 5 # number of epochs since last improvement before stopping training
num_workers: 4 # number of workers to use for data generation, change to 1 for debugging (also moves model to cpu) // MODIFY IF SHARED MEMORY PROBLEM
accelerator: "gpu"
pin_memory: True
learning_rate: 0.01 #"tune.loguniform(0.00001, 0.01)"

# Ray tune parameters
name_experiment: "ray_experiment_0"
n_cpu_per_trials: 4
n_gpu_per_trials: 1
n_sampling: 50

# transforms parameters:
length_segments: 3
sample_rate: 44100 # Value for resampling the audiofile

# For monitoring metrics
mlflow:
  experiment_name: *current_exp
  tracking_uri: mlflow.get_tracking_uri()

# Model parameters
num_target_classes: 2

# Parameters for the performance run
num_samples: 1



